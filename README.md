# ğŸ•·ï¸ Web Scraping Workshop mit Cursor

Ein hands-on Workshop zum Erlernen von Web Scraping mit Scrapy, BigQuery und AI-gestÃ¼tzter Entwicklung.

---

## ğŸ¤” Was ist Web Scraping?

**Web Scraping** ist das automatische Extrahieren von Daten aus Websites. Dein Programm:

1. **Sendet HTTP Requests** (wie ein Browser)
2. **Parst HTML** (analysiert den Quellcode)
3. **Extrahiert Daten** (mit CSS Selektoren)
4. **Speichert Daten** (in Datenbank oder Datei)

### Warum Scrapy?

- âš¡ **Asynchron & Parallel**: Hunderte Requests gleichzeitig
- ğŸ”„ **Built-in Features**: Pagination, Retry, Caching
- ğŸ­ **Production-Ready**: Von Firmen weltweit genutzt

### Deine Mission

Scrape **alle 1000 BÃ¼cher** von [books.toscrape.com](https://books.toscrape.com) und lade sie in **Google BigQuery**!

---

## ğŸ¯ Zwei Modi zur Auswahl

### Option A: Fast Crawler âš¡ (Empfohlen fÃ¼r Einsteiger)

**Ziel**: 1000 BÃ¼cher schnell scrapen

- Nur Ãœbersichtsseiten scrapen (keine Detail-Klicks)
- 50 Seiten durchlaufen
- **Start-URL**: `https://books.toscrape.com/catalogue/category/books_1/index.html`
- **Extrahierte Felder**: 8 von 13
- **Requests**: ~51

---

### Option B: Full Crawler ğŸ” (FÃ¼r Fortgeschrittene)

**Ziel**: 1000 BÃ¼cher mit allen Details

- Jedes Buch einzeln Ã¶ffnen (Detail-Seite)
- ZusÃ¤tzliche Daten aus Produkt-Tabelle extrahieren
- **Start-URL**: `https://books.toscrape.com/`
- **Extrahierte Felder**: Alle 13
- **Requests**: ~1081

---

## ğŸ“Š Welche Daten sollen extrahiert werden?

### Fast Mode: Ãœbersichtsseiten (8 Informationen)

Von jeder Ãœbersichtsseite sollst du folgende Informationen zu jedem Buch extrahieren:

1. **Dein Name** â€“ Damit wir wissen, wer die Daten gescraped hat
2. **Titel des Buches** â€“ Der vollstÃ¤ndige Buchtitel
3. **URL zur Detailseite** â€“ Die vollstÃ¤ndige URL zum Buch
4. **Preis** â€“ Was kostet das Buch? (als Zahl, ohne WÃ¤hrungssymbol)
5. **VerfÃ¼gbarkeit** â€“ Ist das Buch vorrÃ¤tig? (Ja oder Nein)
6. **Bewertung** â€“ Wie viele Sterne hat das Buch? (1-5)
7. **Bild-URL** â€“ Die URL zum Buchcover
8. **Zeitstempel** â€“ Wann wurde dieses Buch gescraped?

---

### Full Mode: Mit Detailseiten (+5 zusÃ¤tzliche Informationen)

ZusÃ¤tzlich zu allen Fast Mode Informationen:

9. **Kategorie** â€“ Zu welcher Kategorie gehÃ¶rt das Buch? (z.B. "Fiction", "Poetry")
10. **Produktcode (UPC)** â€“ Der eindeutige Universal Product Code
11. **Beschreibung** â€“ Der Beschreibungstext des Buches
12. **Anzahl verfÃ¼gbar** â€“ Wie viele Exemplare sind vorrÃ¤tig? (als Zahl)
13. **Anzahl Bewertungen** â€“ Wie viele Bewertungen hat das Buch?

---

### âš ï¸ Wichtig: BigQuery Integration

Die extrahierten Daten mÃ¼ssen in die BigQuery-Tabelle passen!

**Deine Aufgaben:**
- ğŸ” Finde heraus, welche Feldnamen BigQuery erwartet
- ğŸ” Finde heraus, welche Datentypen verwendet werden mÃ¼ssen
- ğŸ” Stelle sicher, dass deine Daten das richtige Format haben

**Tipps:**
```
"Wie kann ich das Schema einer BigQuery-Tabelle abrufen?"
"Wie konvertiere ich einen Preis-String in eine Zahl?"
"Wie erstelle ich einen ISO 8601 Zeitstempel in Python?"
```

---

## â˜ï¸ BigQuery Setup

### Konfiguration (.env)

Die Credentials sind bereits in der `.env` Datei konfiguriert:

```bash
GOOGLE_CLOUD_PROJECT_ID=holding-llm-sichtbarkeit
BIGQUERY_DATASET_ID=keep_learning_scraper
BIGQUERY_TABLE_ID=scraper_results
GOOGLE_APPLICATION_CREDENTIALS=bigquery_credentials.json
```

### Upload-Methode

Dein Scraper soll Daten in **Batches** hochladen (empfohlen: 100 Items pro Request).

---

## ğŸ¤– Mit Cursor arbeiten

### Dein Workflow:

1. **ğŸ¯ Verstehe das Ziel**: Was soll der Scraper machen?
2. **ğŸ” Analysiere die Website**: Ã–ffne [books.toscrape.com](https://books.toscrape.com) im Browser
3. **ğŸ’­ Stelle Cursor Fragen**: Sei spezifisch und klar
4. **ğŸ”„ Iteriere**: Klein anfangen, dann erweitern
5. **âœ… Teste oft**: Nach jeder Ã„nderung testen

### Beispiel-Fragen an Cursor:

```
"Wie kann ich das Schema der BigQuery-Tabelle abrufen?"

"Erstelle ein Scrapy-Projekt fÃ¼r books.toscrape.com"

"Wie finde ich CSS-Selektoren fÃ¼r Buch-Titel auf dieser Website?"

"Wie konvertiere ich 'Â£51.77' zu einem Float-Wert?"

"Wie extrahiere ich eine Zahl aus der CSS-Klasse 'star-rating Three'?"

"Implementiere Pagination, um alle Seiten zu durchlaufen"

"Erstelle eine Scrapy Pipeline, die Daten in Batches zu BigQuery hochlÃ¤dt"
```

### ğŸ’¡ Profi-Tipps:

- **Zeige Cursor die Website**: Ã–ffne die HTML-Struktur mit DevTools (F12)
- **Sei prÃ¤zise**: Nenne exakte Feldnamen und URLs
- **Teste Selektoren**: Nutze `scrapy shell "URL"` zum Testen
- **Frage nach ErklÃ¤rungen**: "Warum funktioniert dieser Selektor?"

---

## ğŸ¯ Deine Challenges

### Challenge 0: BigQuery Schema verstehen â­
- [ ] BigQuery-Tabelle erkunden
- [ ] Feldnamen herausfinden
- [ ] Datentypen verstehen
- [ ] Schema dokumentieren

**Tipp**: Frage Cursor, wie man das Schema einer BigQuery-Tabelle abruft!

---

### Challenge 1: Projekt Setup â­
- [ ] Virtual Environment erstellen
- [ ] Scrapy installieren
- [ ] Scrapy-Projekt initialisieren
- [ ] Ersten Spider erstellen

**Tipp**: Frage Cursor nach dem Setup-Workflow fÃ¼r ein Scrapy-Projekt!

---

### Challenge 2: Erste Daten extrahieren â­â­
- [ ] Spider lÃ¤uft ohne Fehler
- [ ] Mindestens 1 Buch wird gescrapt
- [ ] Titel und Preis werden extrahiert

**Tipp**: Starte mit `scrapy crawl books -o test.json` zum Testen!

---

### Challenge 3: Alle Felder (Fast Mode) â­â­
- [ ] Alle 8 Informationen werden extrahiert (siehe oben)
- [ ] Datentypen entsprechen dem BigQuery-Schema
- [ ] Feldnamen entsprechen dem BigQuery-Schema
- [ ] Dein Name ist als user gespeichert

**Tipp**: Nutze Browser DevTools, um HTML-Struktur zu analysieren!

---

### Challenge 4: Pagination â­â­â­
- [ ] Spider durchlÃ¤uft alle 50 Seiten
- [ ] ~1000 BÃ¼cher werden gescrapt
- [ ] Keine Duplikate

**Tipp**: Suche nach dem "next" Button in der Pagination!

---

### Challenge 5: BigQuery Integration â­â­â­
- [ ] Pipeline erstellt
- [ ] Daten werden in Batches hochgeladen
- [ ] Daten erscheinen in BigQuery-Tabelle

**Tipp**: Frage Cursor nach der `google-cloud-bigquery` Library!

---

### Challenge 6: Full Mode (Optional) â­â­â­â­
- [ ] Spider Ã¶ffnet Detailseiten fÃ¼r jedes Buch
- [ ] ZusÃ¤tzliche 5 Informationen werden extrahiert
- [ ] Alle 13 Felder entsprechen dem BigQuery-Schema

**Tipp**: Nutze `scrapy.Request()` mit `callback` fÃ¼r Detailseiten!

---

## âœ… Testing & Validierung

### Lokaler Test (ohne BigQuery)

```bash
# Virtual Environment aktivieren
source venv/bin/activate

# Spider ausfÃ¼hren (nur erste Seite zum Testen)
scrapy crawl books -o test_output.json -s CLOSESPIDER_PAGECOUNT=1

# Ergebnis prÃ¼fen
cat test_output.json | head -n 30
```

### Test mit BigQuery

```bash
# VollstÃ¤ndiger Crawl mit BigQuery Upload
scrapy crawl books
```

### Validierung in BigQuery

```sql
-- Wie viele BÃ¼cher hast du heute gescrapt?
SELECT 
    user,
    COUNT(*) as book_count,
    MIN(scraped_at) as first_scrape,
    MAX(scraped_at) as last_scrape
FROM `holding-llm-sichtbarkeit.keep_learning_scraper.scraper_results`
WHERE DATE(scraped_at) = CURRENT_DATE()
GROUP BY user
ORDER BY book_count DESC;
```

---

## ğŸ› Troubleshooting

### "Scraped 0 books"
â†’ CSS Selektoren sind falsch. Teste mit `scrapy shell "https://books.toscrape.com"`

### "BigQuery Access Denied"
â†’ PrÃ¼fe, ob `bigquery_credentials.json` existiert

### "TypeError: not JSON serializable"
â†’ PrÃ¼fe Datentypen (Float, Boolean, ISO-String fÃ¼r Timestamp)

### Spider zu langsam
â†’ ErhÃ¶he `CONCURRENT_REQUESTS` in `settings.py`

---

## ğŸ“š NÃ¼tzliche Ressourcen

- **Scrapy Dokumentation**: https://docs.scrapy.org
- **CSS Selektoren Tutorial**: https://www.w3schools.com/cssref/css_selectors.asp
- **BigQuery Python Client**: https://cloud.google.com/python/docs/reference/bigquery/latest
- **Ãœbungs-Website**: https://books.toscrape.com

---

## ğŸ† Leaderboard

Wer scraped die meisten BÃ¼cher? Schau dir die BigQuery-Tabelle an!

---

## ğŸ†˜ Hilfe & Support

**Stuck?** 
1. ğŸ¤– Frage Cursor (zeige ihm die Fehlermeldung)
2. ğŸ‘¨â€ğŸ« Workshop-Leiter fragen
3. ğŸ‘¥ Pair Programming mit Kollegen

---

**Viel Erfolg! ğŸš€**
